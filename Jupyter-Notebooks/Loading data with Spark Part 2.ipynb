{
    "metadata": {
        "kernelspec": {
            "name": "sparkkernel",
            "display_name": "Spark | Scala",
            "language": ""
        },
        "language_info": {
            "name": "scala",
            "mimetype": "text/x-scala",
            "codemirror_mode": "text/x-scala",
            "pygments_lexer": "scala"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "Run the Spark Ingestion Job"
            ],
            "metadata": {
                "azdata_cell_guid": "56ff2631-5a0d-40d6-a459-2049036a7558"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import org.apache.spark.sql.types._\r\n",
                "import org.apache.spark.sql.{SparkSession, SaveMode, Row, DataFrame}\r\n",
                "\r\n",
                "// Change per your installation\r\n",
                "val user= \"admin\"\r\n",
                "val password= \"Password1234\"\r\n",
                "val database =  \"sales\"\r\n",
                "val sourceDir = \"/clickstream_data\"\r\n",
                "val datapool_table = \"web_clickstreams_spark_results\"\r\n",
                "val datasource_name = \"SqlDataPool\"\r\n",
                "val schema = StructType(Seq(\r\n",
                "StructField(\"wcs_click_date_sk\",LongType,true), StructField(\"wcs_click_time_sk\",LongType,true), \r\n",
                "StructField(\"wcs_sales_sk\",LongType,true), StructField(\"wcs_item_sk\",LongType,true),\r\n",
                "StructField(\"wcs_web_page_sk\",LongType,true), StructField(\"wcs_user_sk\",LongType,true)\r\n",
                "))\r\n",
                "\r\n",
                "val hostname = \"master-p-svc\"\r\n",
                "val port = 1433\r\n",
                "val url = s\"jdbc:sqlserver://${hostname}:${port};database=${database};user=${user};password=${password};\""
            ],
            "metadata": {
                "azdata_cell_guid": "0114705a-bf0c-4ed9-88d3-b74a3450be67"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Define and Run the Spark Job"
            ],
            "metadata": {
                "azdata_cell_guid": "66d5e2d1-9610-4df8-b217-ca289a232e59"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import org.apache.spark.sql.{SparkSession, SaveMode, Row, DataFrame}\r\n",
                "\r\n",
                "val df = spark.readStream.format(\"csv\").schema(schema).option(\"header\", true).load(sourceDir)\r\n",
                "val query = df.writeStream.outputMode(\"append\").foreachBatch{ (batchDF: DataFrame, batchId: Long) => \r\n",
                "          batchDF.write\r\n",
                "           .format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
                "           .mode(\"append\")\r\n",
                "            .option(\"url\", url)\r\n",
                "            .option(\"dbtable\", datapool_table)\r\n",
                "            .option(\"user\", user)\r\n",
                "            .option(\"password\", password)\r\n",
                "            .option(\"dataPoolDataSource\",datasource_name).save()\r\n",
                "         }.start()\r\n",
                "\r\n",
                "query.awaitTermination(40000)\r\n",
                "query.stop()"
            ],
            "metadata": {
                "azdata_cell_guid": "8e1ae5d2-c047-425e-8068-8ddb804bccb9"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Query data using Spark"
            ],
            "metadata": {
                "azdata_cell_guid": "2184b411-c980-444d-b2fc-3f03a663a8fc"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def df_read(dbtable: String,\r\n",
                "             url: String,\r\n",
                "             dataPoolDataSource: String=\"\"): DataFrame = {\r\n",
                "     spark.read\r\n",
                "          .format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
                "          .option(\"url\", url)\r\n",
                "          .option(\"dbtable\", dbtable)\r\n",
                "          .option(\"user\", user)\r\n",
                "          .option(\"password\", password)\r\n",
                "          .option(\"dataPoolDataSource\", dataPoolDataSource)\r\n",
                "          .load()\r\n",
                "          }\r\n",
                "\r\n",
                "val new_df = df_read(datapool_table, url, dataPoolDataSource=datasource_name)\r\n",
                "println(\"Number of rows is \" +  new_df.count)"
            ],
            "metadata": {
                "azdata_cell_guid": "c9c6f2a7-7ace-40f7-9ad1-cbd607db4689"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}